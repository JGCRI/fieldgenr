---
title: "Fldgen Tutorial"
author: "Robert Link"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width=8,
  cache=TRUE,
  comment = "#>"
)
library('fldgen')
library('dplyr')
library('ggplot2')
```

The fldgen package allows you to ingest temperature output from an earth system
model (ESM) and generate randomized temperature fields that have the same space
and time correlation properties as the original ESM data.  This tutorial focuses
on how to use the functions in the package to generate and analyze temperature
fields.  The details of how the method works are covered in a companion paper. 
The statistics and analysis from that paper are used as the case study in this
tutorial.

All of the functions used here are documented in R's help system.  Since our
purpose here is to outline what functions have to be called, and in what
sequence, to perform the analysis, we haven't repeated material from the help
files.  If you're confused about how a function is supposed to work, consult the
help files.  For example, `help(read.ncdf)` will print the docs for the function
that reads the netCDF temperature fields.

## Using the field generator
### Reading the ESM data
```{r params}
## parameters for the code below.
ngen <- 4             # number of fields to generate
exfld <- 20           # example field to plot from the time series
set.seed(8675309)     # Set RNG seed so results will be reproducible
siglvl <- 0.01        # significance level for statistical tests
```

All of the data needed for this tutorial is installed with the package.
```{r datadir}
library('fldgen')
datadir <- system.file('extdata', package='fldgen')
```

The ESM temperature field should be in a netCDF file.  The function that reads
it returns a "griddata" structure that contains the data, as well as some
information about the grid, such as the latitude, longitude, and time
coordinates and the vector of area weights needed to compute grid averages.

```{r readdata}
griddata <- read.ncdf(file.path(datadir, 'tann1.nc'))
tgav <- griddata$tas %*% griddata$tgop
```
Here, we have read in the netCDF data and used the global average operator to 
compute a time series of global mean temperatures.  These variables will be used
as input to the functions that analyze the ESM data and produce the temperature
fields.

### Generating fields

The first thing we need is a model for the mean temperature response in each
grid cell.  That is, for each grid cell, what is the mean temperature of that
cell as a function of global mean temperature.  You can use whatever model you
want for this.  We will use a simple linear pattern scaling model, which is
implemented in the `pscl_analyze` function.
```{r pscl}
pscl <- pscl_analyze(griddata$tas, tgav)
```

The mean response analysis should return the model coefficients (called `w` and
`b` in our linear model) and a time series of gridded residuals.  These
residuals encode all of the information about the spatial variability of the
ESM, so it is these we will pass to the empirical orthogonal functions (EOF)
analysis.  

```{r eof}
reof <- eof_analyze(pscl$r, griddata$tgop)
```

The last thing we need to generate our fields is the temporal structure of the
EOF coefficients.  We get this from the Fourier transform of the coordinates of
the residuals in the coordinate system defined by the EOF basis vectors.

```{r fft}
Fx <- mvfft(reof$x)
Fxmag <- abs(Fx)
Fxphase <- atan2(Im(Fx), Re(Fx))
```

Now we're ready to generate fields.  We'll do `r ngen` fields in this example. 
The first one will be a reconstruction of the original input.  The other three
will new fields.

```{r genfields}
tempgrids <- list()
length(tempgrids) <- ngen

## Here tgav is the same as the input, but that doesn't have to be the case
meanfield <- pscl_apply(pscl, tgav)

## First field will have the same phases as the input ESM data
tempgrids[[1]] <- reconst_fields(reof$rotation, mkcorrts(Fxmag, Fxphase), meanfield)

## Other fields will have random phases
for(i in 2:ngen) {
  tempgrids[[i]] <- reconst_fields(reof$rotation, mkcorrts(Fxmag), meanfield)
}

## Subtract off the mean field.  Save these because we will want to use them later
residgrids <- lapply(tempgrids, function(g) {
    g - meanfield
})

## We will also want a grand matrix of all of the generated fields.  We'll leave
## out the one that is an exact reproduction of the input, so as to avoid
## biasing the statistics.
allresidgrids <- do.call(rbind, residgrids[-1])
```

### Plotting the fields

We can extract a single field from each time series and plot them all for
comparision.  We will be able to see a lot more detail if we subtract out the
mean field from each one, so that's what we will do.

```{r plotfields, fig.show='hold', eval=TRUE}
## Extract a single example field from each series and create a plot
fieldplots <- lapply(residgrids, function(g) {
    suppressWarnings(
        plot_field(g[exfld,], griddata, 14)
    )
})

## Display the plots
for (plt in fieldplots) {
    print(plt)
}
```

Incidentally, the second field looks like it's mostly positive deviations from the mean response, but that's just an artifact of the discretization used to emphasize 

## Analysis

### Global mean temperatures

The global mean temperature should be close to zero for all of the residual
fields, but it probably won't be exact.  The differences here tell us something
about the performance of the mean response algorithm.  If the global mean of the
mean response field for input temperature $T_g$ is something other than $T_g$,
then the algorithm isn't consistent.

```{r glblmn}
meantemps <- lapply(seq_along(residgrids), function(i) {
    residgrids[[i]] %*% griddata$tgop
    }) 
for(mt in meantemps)
    print(as.vector(summary(mt)))
```

### Grid cell variance

We also expect the variance in each grid cell to be the same as the variance in
the original ESM data.  We can test this by calculating the $F$ statistic.  We
don't want to do an F-test on each grid cell, and some of them are bound to fail
anyhow, since there are `r dim(griddata$tgop[1])` grid cells.  Instead, we'll
calculate all the $F$ values and see how many would fail at the `r siglvl`
level.  Also, note that `residgrids[[1]]` is identical to the original (this is
tested in the package unit tests).  Therefore, we will compare the other grids
to that one.

```{r fstats}
cellvar <- lapply(residgrids, function(g) {
    apply(g, 2, var)   # calculate variance in each grid cell across time slices.
    })
fstats <- lapply(cellvar[-1], function(cvg) {
    cvg / cellvar[[1]]     # F = var(x) / var(y)
    })

pv <- c(siglvl/2, 1-siglvl/2)      # This is a two-tailed test, so the p-vals are half the sig level
df1 <- length(griddata$time) - 1  # degrees of freedom is N-1
fq <- qf(pv, df1, df1)
fracfail <- sapply(fstats, function(f) {
    sum(f<fq[1] | f>fq[2]) / length(f)
})
print(fracfail)
```

In other words the number of grid cells that appear to show significant
differences in variance is well within what we would expect by chance alone.

An alternate version of this calculation is to look at the variances of the grid
cells in all of the field realizations, treating the variance as an operation
across both time and alternate realizations.  We'll do that here and see what if
any difference it makes.
```{r fstatsall}
fstats.all <- apply(allresidgrids, 2, var) / cellvar[[1]]
df.all <- dim(allresidgrids)[1] - 1    # More degrees of freedom in the merged version
fq.all <- qf(pv, df.all, df.all)
fracfail.all <- sum(fstats.all < fq.all[1] | fstats.all > fq.all[2]) / length(fstats.all)
print(fracfail.all)
```
Once again, the number of grid cells that fail is negligible.  We'll probably
use this version in the paper.


### Grid cell normality

We also need to establish that the data are normally distributed.  (Technically
we should do this before we apply the F-test.)  We'll use the Kolmogorov-Smirnov
statistic for this test.  We don't have a separate quantile function for null
hypothesis distribution of this statistic, so we'll just use the p-values
produced by `ks.test`.

```{r kstest}
kstestpvals <- lapply(residgrids, function(g) {
    apply(g, 2, function(x) {
        mu <- mean(x)
        sig <- sd(x)
        nulldist <- function(q) {pnorm(q, mu, sig)}
        ks.test(x, nulldist)$p.value
    })
})

fracfailks <- sapply(kstestpvals, function(pv) {
    sum(pv < siglvl) / length(pv)
    })
print(fracfailst)
```
The residuals constructed by the Fourier process show no significant evidence of
non-normality.  This is what we would expect, even if the original data were
_not_ normally distributed because the the process is essentially summing up a
collection of random variables, and the Central Limit Theorem tells us that such
sums approach a normal distribution as the number of terms in the sum increases.

As with the variance, we can do this test on the merged dataset.
```{r kstestall}
kspvals.all <- apply(allresidgrids, 2, function(x) {
    mu <- mean(x)
    sig <- sd(x)
    nulldist <- function(q) {pnorm(q, mu, sig)}
    ks.test(x, nulldist)$p.value
})

fracfailks.all <- sum(kspvals.all < siglvl) / length(kspvals.all)
print(fracfailks.all)
summary(kspvals.all)
```
None of the grid cells fails this K-S test.  The smallest $p$-value in the grid
is over 0.02.

